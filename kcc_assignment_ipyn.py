# -*- coding: utf-8 -*-
"""KCC Assignment.ipyn

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xt67Zy_p5b_CSfiKSbdbHpsuQpzfVkYD
"""

import pandas as pd
data = pd.read_csv('KCC_Analytical_Assessment_Data (1).csv')

# STEP 1 in data cleaning:  Check that all data columns are relevant
data.head(5)

"""All columns here appear to be relevant, so can continue to step 2"""

# STEP 2 in data cleaning:  Check to see if there are any duplicated rows:
data.duplicated().sum()

# Explore data types:
type(data)
data.info()

# Step 3 in data cleaning:  Fix any structural errors / ensure data is in correct columns /change columns to lower-case to make our data easier to work with:
# We can change convert lower-case and hyphenate names with 2+ words
data.columns = data.columns.str.lower().str.replace(" ","_")

# Use value-counts/describe or other methods to ensure data in each column is entered as expected:
data.portfolio.value_counts()

data.state.value_counts()

# Explore postal codes, primarily ensuring they are all numerical:
data.postalcode.describe()

data.occupancy_code.value_counts()

data.construction_code.value_counts()

# We notice that the 'Wood' and 'Wood Frame' were accidentally entered as thier descriptions instead of thier codes, so we can replace those values
data.construction_code = data.construction_code.replace('Wood','WD00')
data.construction_code = data.construction_code.replace('Wood Frame','WD10')
data.construction_code.value_counts()

# Continue checking columns for unusual data entries:
data.stories.value_counts()

data.year_built.describe()

# Use graphical analysis to check for outliers in value variables
import plotly.express as px
aal_graph = px.histogram(data, x='aal')
aal_graph

building_value_graph = px.histogram(data, x='building_value')
building_value_graph

other_value_graph = px.histogram(data, x='other_value')
other_value_graph

contents_value_graph = px.histogram(data, x='contents_value')
contents_value_graph

time_element_value_graph = px.histogram(data, x='time_element_value')
time_element_value_graph

"""No unusual data or outliers are detected, so we can continue to the next step of data cleaning:

"""

# STEP 4 in data cleaning:  Find missing column values, if any:
data.isna().sum()

# Exploring options to replace these missing values or decide to drop:
# Create a copy of the data set and drop missing values in the 'stories' and 'year_built' columns:
data2 = data.copy()
data2 = data2.dropna(subset=['stories'])
data2 = data2.dropna(subset=['year_built'])
data2.isna().sum()

# We can check value counts to see if our data follows a distribution.
# If the data looks normally distributed, we can replace missing values with the mean of 'stories' or the mean of 'year_built'
data2['stories'].value_counts()

data2['year_built'].plot(kind='kde')

# Since neither column appear to be normally distributed, we can fill in the most frequent values instead:
# Note: We could have likely just dropped these 3 rows because there are nearly 50,000 observations
most_freq_stories = data.stories.mode()
data.stories = data.stories.fillna(float(most_freq_stories))
most_freq_year = data.year_built.mode()
data.year_built = data.year_built.fillna(float(most_freq_year))

"""Data cleaning is now complete!  We can move to create new columns that will help with our tasked questions.

"""

# Create a new column for TIV and take a sample from the dataset to verify calculations:
data['total_insured_value'] = data['building_value'] + data['other_value'] + data['contents_value'] + data['time_element_value']
data.sample(5)

# Create a new column for risk count (1 at each location)
data['risk_count'] = 1
data.sample(5)

# We can calculate the total insured values and risk counts with the help of Numpy's library:

import numpy as np
risk_by_state = data.groupby(['state']).agg({'risk_count':np.sum,'total_insured_value':np.sum})
risk_by_state.sort_values(ascending=False,by='total_insured_value')

# Create a filter for Wood Frame buildings, and apply this filter to find total insured values by county:
# In case there are two states with the same county name, we also group by state
# (for example, El Paso is a county in both TX and CO, so we do not want these values double-counted)
WD10_filter = data.construction_code == 'WD10'
WD10_table = data.loc[WD10_filter,:]
WD10_TIV_by_county = WD10_table.groupby(['state','county']).agg({'total_insured_value':np.sum})
WD10_TIV_by_county['total_insured_value'].sort_values(ascending=False).head(5)

"""*We find the counties with the highest total insured values for Wood Framed locations are Cook, IL; Harris, TX; Dallas, TX; New York, NY; and El Paso, TX.*

We are next tasked with finding the the top 10 postal codes that are 'most susceptible' to damage from winter storms.
For this task, we can assume that the most susceptible locations have accumulated the most average annual losses historically.
"""

# We can find postal codes that are most suseptible to damage in portfolio 1:
portfolio1_filter = data.portfolio == 1
portfolio1_only = data.loc[portfolio1_filter,:]
aal_by_postal_code = portfolio1_only.groupby(['postalcode']).agg({'aal':np.sum})
aal_by_postal_code['aal'].sort_values(ascending=False).head(10)

# We can find postal codes that are most suseptibe to damage in portfolio 2:
portfolio2_filter = data.portfolio == 2
portfolio2_only = data.loc[portfolio2_filter,:]
aal_by_postal_code = portfolio2_only.groupby(['postalcode']).agg({'aal':np.sum})
aal_by_postal_code['aal'].sort_values(ascending=False).head(10)

"""For our last task, we will proceed by finding the breakdown of total insured value, risk count, and AAL by builing height.

To work with the story entries "5+" and "Over 10," we should replace these values with randomly generated integers from 5 - 20 and 11 to 20, respectively, assuming that the tallest builing the company insures is 20 stories.
"""

import random
random_5_20 = random.randint(5,20)
random_11_20 = random.randint(11,20)
data.stories = data.stories.replace('5+', random_5_20)
data.stories = data.stories.replace('Over 10',random_11_20)

# We need to convert the 'stories' data type to an integer for our conditional logic to operate:
data['stories'] = data['stories'].astype(int)

data['building_height_band'] = None
data['building_height_band'] = np.where((data.stories >=1) & (data.stories <= 3), 'Small', data.building_height_band)
data['building_height_band'] = np.where((data.stories >=4) & (data.stories <= 7), 'Medium', data.building_height_band)
data['building_height_band'] = np.where(data.stories >=8, 'Large', data.building_height_band)
data

# For the next part, we will isolate PA entries:
PA_filter = data.state == "PA"
PA_table = data.loc[PA_filter,:]
aggregatedPA = PA_table.groupby(['building_height_band','postalcode']).agg({'total_insured_value':np.sum,'risk_count':np.sum,'aal':np.sum})
aggregatedPA.to_csv('aggregatedPA.csv')
aggregatedPA

# We can find the totals for TIV, Risk Counts, and AAL below:
TIV_totals = aggregatedPA.groupby(['building_height_band']).agg({'total_insured_value':np.sum})
TIV_totals

RC_totals = aggregatedPA.groupby(['building_height_band']).agg({'risk_count':np.sum})
RC_totals

AAL_totals = aggregatedPA.groupby(['building_height_band']).agg({'aal':np.sum})
AAL_totals

"""The following calculations are used to assess each portfolio's vulnerability to winter storm damage.
We can assume that vulnerability can be calculated as AAL / TIV.  The higher this ratio, the more vulnerable this portfolio is to losses.
"""

# We can add a "vulnerability score" variable to the table that is the ratio of AAL to TIV:
data['vulnerability_score'] = data.aal / data.total_insured_value

vulnerability = data.groupby(['portfolio']).agg({'vulnerability_score':np.sum})
vulnerability

"""Where does this vulnerability stem from?  Perhaps we can find out if it is correlated with one of our other variables:"""

# First, we can add a column for the building age band:
data['building_age_band'] = None
data['building_age_band'] = np.where((data.year_built <= 1980), 'Old', data.building_age_band)
data['building_age_band'] = np.where((data.year_built >1981) & (data.year_built <= 2000), 'Average', data.building_age_band)
data['building_age_band'] = np.where(data.year_built > 2000, 'New', data.building_age_band)

"""A positive correlation will tell us: for a higher variable x, we can expect a higher average annual loss:

A negative correlation will tell us: for a higher a variable x, we can expect a lower average annual loss:
"""

np.corrcoef(data.stories,data.aal)

np.corrcoef(data.year_built,data.aal)

"""Neither of these correlations are strong enough to significanly affect AAL."""

construction_code_analysis = data.groupby(['portfolio','construction_code']).agg({'aal':np.sum,'risk_count':np.sum})
construction_code_analysis

state_analysis = data.groupby(['portfolio','state']).agg({'aal':np.sum,'risk_count':np.sum})
state_analysis

building_height_analysis = data.groupby(['portfolio','building_height_band']).agg({'aal':np.sum})
building_height_analysis

building_age_analysis = data.groupby(['portfolio','building_age_band']).agg({'aal':np.sum})
building_age_analysis

occupancy_code_analysis = data.groupby(['portfolio','occupancy_code']).agg({'aal':np.sum})
occupancy_code_analysis